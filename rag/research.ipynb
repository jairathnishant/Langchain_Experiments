{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bbb1d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c38b74e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\Users\\\\91996\\\\LANGCHAIN\\\\api\\\\rag\\\\speech.txt'}, page_content='Well, I’m going to open up with a quote. It’s one of my favorite quotes. “Actions speak louder than words.” That literally is what defines my career. I made my first Olympic team, 15 years old. I got fifth place. I wasn’t happy. I wasn’t satisfied. They gave me a piece of paper that said, congratulations, you participated. That piece of paper motivated me for that whole next four years. I said, there’s not a shot in hell, this is ever going to happen again. My very first [Olympic] race, I did not medal. And then in 2012, I had one race I didn’t medal. I can go back and look at those races because I want to make sure that feeling stays with me until the next time I have a chance to get out there to do the same thing again. From 2002 to 2008, guess how many days I took off. In those six years, guess how many days I took off. None. Zero. Why? I wanted something that nobody else had the opportunity to get. I was willing to do more than anybody else on the planet was willing to do. I got the results. I wanted to do it because I wanted that chance. Nobody’s going to give you that chance, right? You’re going to have to earn that chance. That’s all I wanted. I wanted to bust my ass every single day to earn that one chance. And wherever that took me, it took me. Every time I’d go into practice, my coach would say jump. I would say how high, because I knew there were hundreds of thousands of other kids that were doing the same thing. And they were not going to take that opportunity away from me. Y’all have one of the greatest, if not the greatest coach leading y’all every single day. He’s got the answers. But it’s y’all that need to listen or making sure you’re doing everything away from the field. Right? It’s not just what happens here. I can’t tell you that enough. It is not just what happens here. It’s the whole entire picture. You get one chance, right? You get once chance to do something special. Don’t waste it, please.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = TextLoader(r\"C:\\Users\\91996\\LANGCHAIN\\api\\rag\\speech.txt\", encoding=\"utf-8\")\n",
    "text_doc = loader.load()\n",
    "text_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21dfa6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\") \n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2306037f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# web based loader\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "# load,chunk and index html page\n",
    "\n",
    "loader = WebBaseLoader(web_paths = [\"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\"],\n",
    "                        bs_kwargs = dict(parse_only = bs4.SoupStrainer(\n",
    "                            class_ = (\"post-title\", \"post-content\", \"post-header\") \n",
    "                        )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbae0529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      Prompt Engineering\n",
      "    \n",
      "Date: March 15, 2023  |  Estimated Reading Time: 21 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\n",
      "This post only focuses on prompt engineering fo\n"
     ]
    }
   ],
   "source": [
    "text_documents = loader.load()\n",
    "print(text_documents[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7061abc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/'}, page_content='Prompt Engineering\\n    \\nDate: March 15, 2023  |  Estimated Reading Time: 21 min  |  Author: Lilian Weng'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/'}, page_content='Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/'}, page_content=\"[My personal spicy take] In my opinion, some prompt engineering papers are not worthy 8 pages long, since those tricks can be explained in one or a few sentences and the rest is all about benchmarking. An easy-to-use and shared benchmark infrastructure should be more beneficial to the community. Iterative prompting or external tool use would not be trivial to set up. Also non-trivial to align the whole research community to adopt it.\\nBasic Prompting#\\nZero-shot and few-shot learning are two most basic approaches for prompting the model, pioneered by many LLM papers and commonly used for benchmarking LLM performance.\\nZero-Shot#\\nZero-shot learning is to simply feed the task text to the model and ask for results.\\n(All the sentiment analysis examples are from SST-2)\\nText: i'll bet the video game is a lot more fun than the film.\\nSentiment:\\nFew-shot#\"),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/'}, page_content=\"(All the sentiment analysis examples are from SST-2)\\nText: i'll bet the video game is a lot more fun than the film.\\nSentiment:\\nFew-shot#\\nFew-shot learning presents a set of high-quality demonstrations, each consisting of both input and desired output, on the target task. As the model first sees good examples, it can better understand human intention and criteria for what kinds of answers are wanted. Therefore, few-shot learning often leads to better performance than zero-shot. However, it comes at the cost of more token consumption and may hit the context length limit when input and output text are long.\\nText: (lawrence bounces) all over the stage, dancing, running, sweating, mopping his face and generally displaying the wacky talent that brought him fame in the first place.\\nSentiment: positive\"),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/'}, page_content=\"Text: despite all evidence to the contrary, this clunker has somehow managed to pose as an actual feature movie, the kind that charges full admission and gets hyped on tv and purports to amuse small children and ostensible adults.\\nSentiment: negative\\n\\nText: for the first time in years, de niro digs deep emotionally, perhaps because he's been stirred by the powerful work of his co-stars.\\nSentiment: positive\")]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(text_documents)\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f556e0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its co'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1].page_content[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f7d5247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91996\\AppData\\Local\\Temp\\ipykernel_8000\\3118289876.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "c:\\Users\\91996\\LANGCHAIN\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "db = Chroma.from_documents(documents[:50], embeddings)\n",
    "# db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6328e42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getenv(\"CHROMA_DB_DIR\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "219e6790",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is instruction prompting?\"\n",
    "result = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23720196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Instruction Prompting#\\nThe purpose of presenting few-shot examples in the prompt is to explain our intent to the model; in other words, describe the task instruction to the model in the form of demonstrations. However, few-shot can be expensive in terms of token usage and restricts the input length due to limited context length. So, why not just give the instruction directly?\\nInstructed LM (e.g. InstructGPT, natural instruction) finetunes a pretrained model with high-quality tuples of (task instruction, input, ground truth output) to make LM better understand user intention and follow instruction. RLHF (Reinforcement Learning from Human Feedback) is a common method to do so. The benefit of instruction following style fine-tuning improves the model to be more aligned with human intention and greatly reduces the cost of communication.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f9a2f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91996\\AppData\\Local\\Temp\\ipykernel_8000\\1506236888.py:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model = \"llama2\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ollama()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model = \"llama2\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fd33d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design ChatPrompt template\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context.\n",
    "Think step by step before providing a detailed answer.\n",
    "If you don't know the answer then return \"I don't know\".\n",
    "<context>\n",
    "{context}\n",
    "</conetxt>\n",
    "Question: {input}\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f03b9ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain introduction\n",
    "# Create stuff document chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45fea82f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001E2B7262610>, search_kwargs={})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining Retriever\n",
    "\n",
    "retriever=db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4677340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever + Chain\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "071759bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How is few shot prompting different from zero shot?',\n",
       " 'context': [Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/'}, page_content='Instruction Prompting#\\nThe purpose of presenting few-shot examples in the prompt is to explain our intent to the model; in other words, describe the task instruction to the model in the form of demonstrations. However, few-shot can be expensive in terms of token usage and restricts the input length due to limited context length. So, why not just give the instruction directly?\\nInstructed LM (e.g. InstructGPT, natural instruction) finetunes a pretrained model with high-quality tuples of (task instruction, input, ground truth output) to make LM better understand user intention and follow instruction. RLHF (Reinforcement Learning from Human Feedback) is a common method to do so. The benefit of instruction following style fine-tuning improves the model to be more aligned with human intention and greatly reduces the cost of communication.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/'}, page_content=\"(All the sentiment analysis examples are from SST-2)\\nText: i'll bet the video game is a lot more fun than the film.\\nSentiment:\\nFew-shot#\\nFew-shot learning presents a set of high-quality demonstrations, each consisting of both input and desired output, on the target task. As the model first sees good examples, it can better understand human intention and criteria for what kinds of answers are wanted. Therefore, few-shot learning often leads to better performance than zero-shot. However, it comes at the cost of more token consumption and may hit the context length limit when input and output text are long.\\nText: (lawrence bounces) all over the stage, dancing, running, sweating, mopping his face and generally displaying the wacky talent that brought him fame in the first place.\\nSentiment: positive\"),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/'}, page_content=\"[My personal spicy take] In my opinion, some prompt engineering papers are not worthy 8 pages long, since those tricks can be explained in one or a few sentences and the rest is all about benchmarking. An easy-to-use and shared benchmark infrastructure should be more beneficial to the community. Iterative prompting or external tool use would not be trivial to set up. Also non-trivial to align the whole research community to adopt it.\\nBasic Prompting#\\nZero-shot and few-shot learning are two most basic approaches for prompting the model, pioneered by many LLM papers and commonly used for benchmarking LLM performance.\\nZero-Shot#\\nZero-shot learning is to simply feed the task text to the model and ask for results.\\n(All the sentiment analysis examples are from SST-2)\\nText: i'll bet the video game is a lot more fun than the film.\\nSentiment:\\nFew-shot#\"),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/'}, page_content='Few-shot CoT. It is to prompt the model with a few demonstrations, each containing manually written (or model-generated) high-quality reasoning chains.')],\n",
       " 'answer': \"Great, I'm here to help you with your question! Based on the provided context, I can confidently answer that few-shot prompting is different from zero-shot prompting in the following ways:\\n\\n1. Number of examples: Few-shot prompting involves providing a small number of examples (typically 3-5) to the model, while zero-shot prompting does not provide any examples before asking for the model's output.\\n2. Quality of examples: In few-shot prompting, the examples provided are high-quality and manually written or generated by the researcher, while in zero-shot prompting, the examples are simply the task text fed directly to the model without any additional context or guidance.\\n3. Model understanding: Few-shot prompting is designed to help the model understand human intention and criteria for what kinds of answers are wanted, as the model sees good examples that demonstrate the desired output. In contrast, zero-shot prompting does not provide any such examples, so the model may not have a clear understanding of what kind of output is expected.\\n4. Performance: Few-shot learning often leads to better performance than zero-shot learning due to the model's ability to better understand human intention and criteria. However, it comes at the cost of more token consumption and may hit the context length limit when input and output text are long.\\n\\nIn summary, few-shot prompting is a more structured and guided approach to prompting the model, while zero-shot prompting is a more straightforward and direct approach that does not provide any additional context or guidance to the model.\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke({\"input\": \"How is few shot prompting different from zero shot?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
